---
layout: post
category: fastai-curr
title: Summer Smackdown - Week 2
---

**News:** the NLP course is officially out now, so I have been watching the videos when the topic needs further clarification. You can see the blogpost here [New NLP](https://www.fast.ai/2019/07/08/fastai-nlp/)

---

Computational Linear Algebra
---





---

Matrix Calulus
---


---

Natural Language Processing
---

The rest of the lecture was focused on getting word counts and tokens. This used the sklearn
library and nltk to create a 'vectorizer'. What this accomplishes is taking the 2,034 posts and getting a total word count overall.

{% highlight python3 %}
{% raw %}
vectorizer = CountVectorizer(stop_words='english')

vectors = vectorizer.fit_transform(newsgroups_train.data).todense()
{% endraw %}
{% endhighlight %}

This will show us that our posts have a total of roughly 26,000 words. From there, we can gather those unique words using a function called `get_feature_names` and we can take a look at what's going on under the hood. The results? A dictionary! (Which doesn't surprise us in the slightest)

Next we looked at SVD. A few things I noted down:

"We should expect words that appear more frequently in one topic to not appear frequently in another, thus the topics should be **orthogonal** (invarient)

SVD factorizes a matrix into a matrix, with **orthognal columns** and **orthogonal rows** along with a diagonal matrix which contains the **relative importance** of each factor (bias)"

Along with this is this wonderful visualization:
![](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/svd_fb.png)

What does it mean? So we have "U", which is our left vectors (also known as our **orthogonal columns**!). Next we have Sigma, which is our bias in relation to **relative importance**, and then lastly we have "V^t", which is our **orthogonal rows**

What is orthogonal? Orthogonal is when we would expect words that appear more frequently in one topic and not in another, we could consider the topics **orthogonal**, or invarient.

Since these matrices that are created (A) are large enough to fully cover the original, this is called **exact decomposition**, which has many use cases including semantic analysis, collaborative filtering, and data compression.

Next we generated a small function that allowed us to see the topics of what our model generated, and it was found that the SVD could figure out pretty well how everything was grouped, subject-wise:

{% highlight python3 %}
{% raw %}
['ditto critus propagandist surname galacticentric kindergarten surreal imaginative',
 'jpeg gif file color quality image jfif format',
 'graphics edu pub mail 128 3d ray ftp',
 'jesus god matthew people atheists atheism does graphics',
 'image data processing analysis software available tools display',
 'god atheists atheism religious believe religion argument true',
 'space nasa lunar mars probe moon missions probes',
 'image probe surface lunar mars probes moon orbit',
 'argument fallacy conclusion example true ad argumentum premises',
 'space larson image theory universe physical nasa material']
{% endraw %}
{% endhighlight %}

Then we got into "Non-Negative Matrix Factorization", or NMF. Instead of orthogonal like before, we instead make all the vectors be non-negative. Our new equation looks like so:

V = W*H

Where **W** and **H** are non-negative vectors. The reasoning for this is positive factors are much more easily interpretable by users and thus this is generally more popular today.

This has many variations and use cases including colaborativefiltering, facial decomposition, and even in bioinformatics.

Take the following photo:
![](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/nmf_doc.png)

All we are doing is a W*H matrix multiplication to give us V


Sklearn has a library for us to use NMF with decomposition, and we can perform the following:

{% highlight python3 %}
{% raw %}
clf = decomposition.NMF(n_components=d, random_state=1)
W1 = clf.fit_transform(vectors)
H1 = clf.components_
{% endraw %}
{% endhighlight %}

This will give us the following relative topic keywords:
{% highlight python3 %}
{% raw %}
['jpeg image gif file color images format quality',
 'edu graphics pub mail 128 ray ftp send',
 'space launch satellite nasa commercial satellites year market',
 'jesus god people matthew atheists does atheism said',
 'image data available software processing ftp edu analysis']
 {% endraw %}
{% endhighlight %}

 I'd argue this is a *lot* better than what we were looking at earlier!

The last algorithm we looked at was Topic Frequence-Inverse Document Frequency, TF-IDF. With this algorithm, we normalize the term counts by using how often each term appears, how long it is, and how common or rare the term is.

We utilize the following formulas:

TF = (# of occurences of term *t* in the document) / (# of words in the document)

IDF = -log(# of documents / # of documents with term *t* in it)

Using this new algorithm, we can fit again and get the following results;

{% highlight python3 %}
{% raw %}
['people don think just like objective say morality',
 'graphics thanks files image file program windows know',
 'space nasa launch shuttle orbit moon lunar earth',
 'ico bobbe tek beauchaine bronx manhattan sank queens',
 'god jesus bible believe christian atheism does belief']
{% endraw %}
{% endhighlight %}

Much more context is found here and we start to see almost complete thoughts as well. 

**Why randomized SVD's:**

Randomized algorithms are inheriently more stable as we do not depend on subtle properties at startup, and the products can be done in parallel. 

Each algorithm has a loss and a benefit associated with them, the standard numpy SVD takes a long time (~1.16 minutes), wheras sklearn's randomized SVD takes 8.88 seconds.

The last one Rachel showed is a Randomized SVD from Facebook's library, 'fbpca'. This one by far had the fastest time, clocking in at a whopping 3 seconds.

**Possible Capstone?**

Rachel also shows us another approach to tokenization, through [SentencePiece](https://github.com/google/sentencepiece), which seems to be new and state of the art out from Google. I may try to implement this and try it with ULM-FiT or the standardvectorization's we saw today.

---

Deep Learning from the Foundations
---